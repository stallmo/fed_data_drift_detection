import sys
import pickle
import datetime
import logging

import numpy as np

from helpers import generator_helpers as gen_help

sys.path.append('../cluster_library')
sys.path.append('../')

import cluster_validation.davies_bouldin as db
import federated_clustering.local_learners as fcll
import federated_clustering.global_learner as fcgl

if __name__ == '__main__':

    ###### Set parameters for experiment ######
    logging.basicConfig(level=logging.INFO)

    save_results = True  # whether to save results dictionary as pickle file
    log_mlflow = True  # whether to log experiment results to mlflow

    run_mode = 'no_local_drift'  # 'no_local_drift'  # 'local_but_no_global_drift' # global_drift # 'global_drift_unused_generators'
    n_repeats = 20  # number of times the experiment is repeated
    n_time_steps = 10  # number of time steps in each repeat

    acceptability_threshold = 0.01  # 0.05 means 5% deviation from initial Davies-Bouldin is acceptable -> delta

    n_clients = 10  # number of clients participating in the federation
    ratio_unused_client_generators = 0.0  # ratio of clients of which data generators are not used -> only global drift with unused generators
    ratio_drifted_clients = 0.5  # only relevant for global drift. ratio of clients that drift
    drifting_clients = int(n_clients * ratio_drifted_clients)
    n_unused_client_generators = int(n_clients * ratio_unused_client_generators) # number of clients of which data generators are not used -> only global drift
    ratio_new_data_distribution = 0.2  # how many of the new data points are generated by new distributions. rest is generated by old distributions -> rho
    dim_data = 2  # dimension of data points
    n_generators_per_client = 1  # number of gaussian distributions per client
    initial_points_per_generator = 1000  # number of initial data points per generator
    next_timestep_new_points_per_generator = 100  # number of new data points per generator in each time step
    mean_min_val = -5  # minimum value for random mean of gaussian data generators
    mean_max_val = 5  # maximum value for random mean of gaussian data generators
    cov_mat_initial = np.eye(dim_data) * 0.5  # we make sure the initial data is clusterable

    # fuzzy c-means parameters
    m = 2  # fuzziness parameter
    tol_local = 0.001  # tolerance for local clustering
    max_iter_local = 50  # maximum number of iterations for local clustering
    max_iter_global = 10  # maximum number of iterations for global clustering
    tol_global = 0.01  # tolerance for global clustering
    ideal_cluster_num_deviation = 0.0  # determines deviation from ideal number of clusters. 0.0 means ideal number of clusters is used. Can be positive or negative.
    ideal_cluster_num = n_generators_per_client * n_clients

    if ideal_cluster_num_deviation == 0.0:
        num_clusters = ideal_cluster_num  # we assume the number of (global) clusters to be give at this point
    else:
        num_clusters = int(np.round(ideal_cluster_num * (1 + ideal_cluster_num_deviation)))

    ###### Here, the actual experiments start ######

    # create a dictionary to keep track of experimental results
    dict_experiments_results = {'config': {'run_mode': run_mode,
                                           'n_repeats': n_repeats,
                                           'n_clients': n_clients,
                                           'ratio_drifted_clients': ratio_drifted_clients,
                                           'drifting_clients': drifting_clients,
                                           'ratio_new_data_distribution': ratio_new_data_distribution,
                                           'dim_data': dim_data,
                                           'n_generators_per_client': n_generators_per_client,
                                           'initial_points_per_generator': initial_points_per_generator,
                                           'next_timestep_new_points_per_generator': next_timestep_new_points_per_generator,
                                           'mean_min_val': mean_min_val,
                                           'mean_max_val': mean_max_val,
                                           'cov_mat_initial': cov_mat_initial,
                                           'm': m,
                                           'tol_local': tol_local,
                                           'max_iter_local': max_iter_local,
                                           'max_iter_global': max_iter_global,
                                           'tol_global': tol_global,
                                           'num_clusters': num_clusters,
                                           'acceptability_threshold': acceptability_threshold,
                                           'ideal_cluster_num_deviation': ideal_cluster_num_deviation,
                                           'ideal_cluster_num': ideal_cluster_num,
                                           'ratio_unused_client_generators': ratio_unused_client_generators,
                                           'n_unused_client_generators': n_unused_client_generators
                                           },
                                'experiments': {}
                                }

    # depending on run_mode, get new data from generators for each client
    list_all_recalculated_davies_bouldin = []
    drift_detected_counter = 0

    # repeat experiment n_repeats times
    for repeat in range(n_repeats):
        print(f'Starting repeat {repeat + 1} of {n_repeats}.')
        print('*' * 10)

        # we start with creating new initial data and cluster model

        # create initial datasets at t0
        dict_initial_data_generators_per_client = {}
        list_local_learners = []

        for client in range(n_clients):
            # each client has specified number of initial gaussian distributions
            client_generators = [gen_help.make_gaussian_data_generator_random_mean(
                min_val=mean_min_val,
                max_val=mean_max_val,
                cov_mat=cov_mat_initial
            ) for _ in
                range(n_generators_per_client)]
            dict_initial_data_generators_per_client[client] = client_generators

            X_initial_client = np.concatenate(
                [generator.generate_data_points(initial_points_per_generator, seed=43) for generator in
                 client_generators])
            # Setup federation
            local_learner = fcll.FuzzyCMeansClient(client_data=X_initial_client,
                                                   num_clusters=num_clusters,
                                                   max_iter=max_iter_local,
                                                   tol=tol_local, m=m
                                                   )
            list_local_learners.append(local_learner)

        # learn global federated fuzzy c-means model
        global_learner = fcgl.GlobalClusterer(local_learners=list_local_learners,
                                              num_clusters=num_clusters,
                                              data_dim=dim_data,
                                              max_rounds=max_iter_global,
                                              tol=tol_global,
                                              weighing_function=None,
                                              global_center_update='kmeans')

        global_learner.fit()
        global_cluster_centers = global_learner.cluster_centers

        # calculate global fuzzy-DB and calculate acceptable range
        initial_global_db = db.calculate_federated_fuzzy_db(_local_learners=list_local_learners,
                                                            _num_clusters=num_clusters
                                                            )
        lower_bound_db = initial_global_db * (1 - acceptability_threshold)
        upper_bound_db = initial_global_db * (1 + acceptability_threshold)

        print(f'Initial federated fuzzy Davies-Bouldin: {initial_global_db}.')

        tmp_update_dict = {repeat: {'initial_cluster_results': {'global_db': initial_global_db,
                                                                'global_cluster_centers': global_cluster_centers,
                                                                'lower_bound_db': lower_bound_db,
                                                                'upper_bound_db': upper_bound_db
                                                                }
                                    }
                           }
        dict_experiments_results['experiments'].update(tmp_update_dict)

        for time_step in range(n_time_steps):

            drift_detected = False
            dict_new_data_per_client = gen_help.generate_new_data(run_mode=run_mode,
                                                                  n_points_per_generator=next_timestep_new_points_per_generator,
                                                                  dict_init_generators_per_client=dict_initial_data_generators_per_client,
                                                                  data_dim=dim_data,
                                                                  mean_min_val=mean_min_val,
                                                                  mean_max_val=mean_max_val,
                                                                  clients_global_drift=drifting_clients,
                                                                  ratio_new_data_distribution=ratio_new_data_distribution,
                                                                  n_unused_client_generators=n_unused_client_generators
                                                                  )
            # ... and set new data
            for client, local_learner in enumerate(list_local_learners):
                local_learner.set_new_data(dict_new_data_per_client.get(client))

                logging.debug(f'Client {client} has {local_learner.client_data.shape[0]} data points.')

            # with new data, recalculate global Davies-Bouldin
            recalculated_global_db = db.calculate_federated_fuzzy_db(list_local_learners,
                                                                     num_clusters)
            list_all_recalculated_davies_bouldin.append(recalculated_global_db)
            print(f'Iteration {time_step + 1} - Recalculated federated fuzzy Davies-Bouldin: {recalculated_global_db}')

            # Hypothesis testing: test whether recalculated Davies-Bouldin is within acceptable range
            if recalculated_global_db < lower_bound_db or recalculated_global_db > upper_bound_db:
                print(
                    f'Hypothesis test failed. Recalculated Davies-Bouldin is outside acceptable range: ({lower_bound_db}, {upper_bound_db}). Drift detected.')
                drift_detected_counter += 1
                drift_detected = True

            # save experiment in dictionary
            tmp_update_dict = {time_step: {'recalculated_db': recalculated_global_db,
                                           'drift_detected': drift_detected,
                                           'new_data_per_client': dict_new_data_per_client
                                           }
                               }

            dict_experiments_results['experiments'][repeat].update(tmp_update_dict)

    # output how often drift was detected
    print(f'Drift detected {drift_detected_counter} times out of {n_time_steps * n_repeats} iterations.')

    # add drift_detected_counter to dictionary
    dict_experiments_results['drift_detected_counter'] = drift_detected_counter

    # log experiment results and parameters to mlflow
    if log_mlflow:
        import mlflow

        mlflow.set_tracking_uri('http://localhost:8080')
        mlflow.set_experiment(f'data_drift_experiments_{run_mode}')
        with mlflow.start_run():

            mlflow.log_param('run_mode', run_mode)
            mlflow.log_param('n_repeats', n_repeats)
            mlflow.log_param('n_time_steps', n_time_steps)
            mlflow.log_param('n_clients', n_clients)
            mlflow.log_param('ratio_drifted_clients', ratio_drifted_clients)
            mlflow.log_param('drifting_clients', drifting_clients)
            mlflow.log_param('acceptability_threshold', acceptability_threshold)
            mlflow.log_param('ratio_unused_client_generators', ratio_unused_client_generators)
            mlflow.log_param('ratio_new_data_distribution', ratio_new_data_distribution)
            mlflow.log_param('dim_data', dim_data)
            mlflow.log_param('n_generators_per_client', n_generators_per_client)
            mlflow.log_param('initial_points_per_generator', initial_points_per_generator)
            mlflow.log_param('next_timestep_new_points_per_generator', next_timestep_new_points_per_generator)
            mlflow.log_param('mean_min_val', mean_min_val)
            mlflow.log_param('mean_max_val', mean_max_val)

            # mlflow.log_artifact(cov_mat_initial)
            mlflow.log_param('m', m)
            mlflow.log_param('tol_local', tol_local)
            mlflow.log_param('max_iter_local', max_iter_local)
            mlflow.log_param('max_iter_global', max_iter_global)
            mlflow.log_param('tol_global', tol_global)
            mlflow.log_param('ideal_cluster_num_deviation', ideal_cluster_num_deviation)
            mlflow.log_param('num_clusters_used', num_clusters)
            log_mlflow.log_param('ideal_cluster_num', ideal_cluster_num)

            mlflow.log_metric('drift_detected_counter', drift_detected_counter)

            all_global_dbs = [
                dict_experiments_results.get('experiments').get(repeat).get('initial_cluster_results').get('global_db')
                for repeat in range(n_repeats)]

            mlflow.log_metric('initial_global_db_mean', np.mean(all_global_dbs))
            mlflow.log_metric('initial_global_db_median', np.median(all_global_dbs))
            mlflow.log_metric('initial_global_db_std', np.std(all_global_dbs))
            mlflow.log_metric('initial_global_db_min', np.min(all_global_dbs))
            mlflow.log_metric('initial_global_db_max', np.max(all_global_dbs))

            #mlflow.log_metric('initial_global_db', initial_global_db)
            # mlflow.log_artifact(dict_experiments_results)

            if run_mode in ['no_local_drift', 'local_but_no_global_drift']:
                false_positive_rate = drift_detected_counter / (n_repeats*n_time_steps)
                mlflow.log_metric('false_positive_rate', false_positive_rate)
            elif run_mode == 'global_drift':
                true_positive_rate = drift_detected_counter / (n_repeats*n_time_steps)
                mlflow.log_metric('true_positive_rate', true_positive_rate)

    # save experiment results as pickle file
    if save_results:
        now = datetime.datetime.now()
        now_str = now.strftime("%Y-%m-%d_%H-%M-%S")
        filename = f'./results/{run_mode}_{now_str}_results.pkl'
        with open(filename, 'wb') as f:
            pickle.dump(dict_experiments_results, f)
        print(f'Saved results to {filename}.')
