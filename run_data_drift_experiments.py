import sys
import pickle
import datetime
import logging

import numpy as np

from helpers import generator_helpers as gen_help

sys.path.append('../cluster_library')
sys.path.append('../')

import cluster_validation.davies_bouldin as db
import federated_clustering.local_learners as fcll
import federated_clustering.global_learner as fcgl


def run_data_drift_experiments(run_mode='global_drift',
                               acceptability_threshold=0.025,
                               dim_data=2,
                               ideal_cluster_num_deviation=0.0,
                               ratio_unused_client_generators=0.0,
                               ratio_new_data_distribution=0.1,
                               n_repeats=20,
                               n_time_steps=10,
                               n_clients=10,
                               ratio_drifted_clients=0.5,
                               n_generators_per_client=1,
                               initial_points_per_generator=1000,
                               next_timestep_new_points_per_generator=100,
                               mean_min_val=-5,
                               mean_max_val=5,
                               cov_mat_initial_scale=0.5,
                               save_results=True,
                               log_mlflow=True
                               ):
    """
    Function that runs the data drift experiments.
    :param run_mode: Specify experiment run mode. Can be one of ['global_drift', 'no_local_drift', 'local_but_no_global_drift', 'global_drift_unused_generators']
    :param acceptability_threshold: Specify acceptability threshold.
    :param dim_data: Specify dimension of data points.
    :param ideal_cluster_num_deviation: Specify deviation from ideal number of clusters. Must be between -1 and 1.
    :param ratio_unused_client_generators: Specify ratio of clients of which data generators are not used. Only relevant for global drift scenarios.
    :param ratio_new_data_distribution: Specify ratio of new data points generated by new distributions. Only relevant for global drift scenarios.
    :param n_repeats: Specify number of times the experiment is repeated.
    :param n_time_steps: Specify number of time steps in each repeat.
    :param n_clients: Specify number of clients participating in the federation.
    :param ratio_drifted_clients: Specify ratio of clients that drift. Only relevant for global drift scenarios.
    :param n_generators_per_client: Specify number of gaussian distributions per client.
    :param initial_points_per_generator: Specify number of initial data points per generator.
    :param next_timestep_new_points_per_generator: Specify number of new data points per generator in each time step.
    :param mean_min_val: Specify minimum value for random mean of gaussian data generators.
    :param mean_max_val: Specify maximum value for random mean of gaussian data generators.
    :param cov_mat_initial_scale: Specify scaling factor of covariance matrix for initial data.
    :param save_results: Specify whether to save results dictionary as pickle file.
    :param log_mlflow: Specify whether to log experiment results to mlflow. Assumes mlflow server is running on localhost:8080.
    :return: None
    """
    ##### Experiment parameters (derived) #####
    drifting_clients = int(n_clients * ratio_drifted_clients)
    n_unused_client_generators = int(n_clients * ratio_unused_client_generators) # number of clients of which data generators are not used -> only global drift
    # if the run_mode is global_drift_unused_generators, then the number of unused client generators must be at least 1
    if run_mode == 'global_drift_unused_generators':
        n_unused_client_generators = max(n_unused_client_generators, 1)
    cov_mat_initial = np.eye(dim_data) * cov_mat_initial_scale  # we make sure the initial data is clusterable


    # fuzzy c-means parameters
    m = 2  # fuzziness parameter
    tol_local = 0.001  # tolerance for local clustering
    max_iter_local = 50  # maximum number of iterations for local clustering
    max_iter_global = 10  # maximum number of iterations for global clustering
    tol_global = 0.01  # tolerance for global clustering

    ideal_cluster_num = n_generators_per_client * n_clients
    if ideal_cluster_num_deviation == 0.0:
        num_clusters = ideal_cluster_num  # we assume the number of (global) clusters to be give at this point
    else:
        num_clusters = int(np.round(ideal_cluster_num * (1 + ideal_cluster_num_deviation)))

    ###### Here, the actual experiments start ######

    # create a dictionary to keep track of experimental results
    dict_experiments_results = {'config': {'run_mode': run_mode,
                                           'n_repeats': n_repeats,
                                           'n_clients': n_clients,
                                           'ratio_drifted_clients': ratio_drifted_clients,
                                           'drifting_clients': drifting_clients,
                                           'ratio_new_data_distribution': ratio_new_data_distribution,
                                           'dim_data': dim_data,
                                           'n_generators_per_client': n_generators_per_client,
                                           'initial_points_per_generator': initial_points_per_generator,
                                           'next_timestep_new_points_per_generator': next_timestep_new_points_per_generator,
                                           'mean_min_val': mean_min_val,
                                           'mean_max_val': mean_max_val,
                                           'cov_mat_initial': cov_mat_initial,
                                           'm': m,
                                           'tol_local': tol_local,
                                           'max_iter_local': max_iter_local,
                                           'max_iter_global': max_iter_global,
                                           'tol_global': tol_global,
                                           'num_clusters': num_clusters,
                                           'acceptability_threshold': acceptability_threshold,
                                           'ideal_cluster_num_deviation': ideal_cluster_num_deviation,
                                           'ideal_cluster_num': ideal_cluster_num,
                                           'ratio_unused_client_generators': ratio_unused_client_generators,
                                           'n_unused_client_generators': n_unused_client_generators
                                           },
                                'experiments': {}
                                }

    # depending on run_mode, get new data from generators for each client
    list_all_recalculated_davies_bouldin = []
    drift_detected_counter = 0

    # repeat experiment n_repeats times
    for repeat in range(n_repeats):
        print(f'Starting repeat {repeat + 1} of {n_repeats}.')
        print('*' * 10)

        # we start with creating new initial data and cluster model

        # create initial datasets at t0
        dict_initial_data_generators_per_client = {}
        list_local_learners = []

        for client in range(n_clients):
            # each client has specified number of initial gaussian distributions
            client_generators = [gen_help.make_gaussian_data_generator_random_mean(
                min_val=mean_min_val,
                max_val=mean_max_val,
                cov_mat=cov_mat_initial
            ) for _ in
                range(n_generators_per_client)]
            dict_initial_data_generators_per_client[client] = client_generators

            X_initial_client = np.concatenate(
                [generator.generate_data_points(initial_points_per_generator, seed=43) for generator in
                 client_generators])
            # Setup federation
            local_learner = fcll.FuzzyCMeansClient(client_data=X_initial_client,
                                                   num_clusters=num_clusters,
                                                   max_iter=max_iter_local,
                                                   tol=tol_local, m=m
                                                   )
            list_local_learners.append(local_learner)

        # learn global federated fuzzy c-means model
        global_learner = fcgl.GlobalClusterer(local_learners=list_local_learners,
                                              num_clusters=num_clusters,
                                              data_dim=dim_data,
                                              max_rounds=max_iter_global,
                                              tol=tol_global,
                                              weighing_function=None,
                                              global_center_update='kmeans')

        global_learner.fit()
        global_cluster_centers = global_learner.cluster_centers

        # calculate global fuzzy-DB and calculate acceptable range
        initial_global_db = db.calculate_federated_fuzzy_db(_local_learners=list_local_learners,
                                                            _num_clusters=num_clusters
                                                            )
        lower_bound_db = initial_global_db * (1 - acceptability_threshold)
        upper_bound_db = initial_global_db * (1 + acceptability_threshold)

        print(f'Initial federated fuzzy Davies-Bouldin: {initial_global_db}.')

        tmp_update_dict = {repeat: {'initial_cluster_results': {'global_db': initial_global_db,
                                                                'global_cluster_centers': global_cluster_centers,
                                                                'lower_bound_db': lower_bound_db,
                                                                'upper_bound_db': upper_bound_db
                                                                }
                                    }
                           }
        dict_experiments_results['experiments'].update(tmp_update_dict)

        for time_step in range(n_time_steps):

            drift_detected = False
            dict_new_data_per_client = gen_help.generate_new_data(run_mode=run_mode,
                                                                  n_points_per_generator=next_timestep_new_points_per_generator,
                                                                  dict_init_generators_per_client=dict_initial_data_generators_per_client,
                                                                  data_dim=dim_data,
                                                                  mean_min_val=mean_min_val,
                                                                  mean_max_val=mean_max_val,
                                                                  clients_global_drift=drifting_clients,
                                                                  ratio_new_data_distribution=ratio_new_data_distribution,
                                                                  n_unused_client_generators=n_unused_client_generators
                                                                  )
            # ... and set new data
            for client, local_learner in enumerate(list_local_learners):
                local_learner.set_new_data(dict_new_data_per_client.get(client))

                logging.debug(f'Client {client} has {local_learner.client_data.shape[0]} data points.')

            # with new data, recalculate global Davies-Bouldin
            recalculated_global_db = db.calculate_federated_fuzzy_db(list_local_learners,
                                                                     num_clusters)
            list_all_recalculated_davies_bouldin.append(recalculated_global_db)
            print(f'Iteration {time_step + 1} - Recalculated federated fuzzy Davies-Bouldin: {recalculated_global_db}')

            # Hypothesis testing: test whether recalculated Davies-Bouldin is within acceptable range
            if recalculated_global_db < lower_bound_db or recalculated_global_db > upper_bound_db:
                print(
                    f'Hypothesis test failed. Recalculated Davies-Bouldin is outside acceptable range: ({lower_bound_db}, {upper_bound_db}). Drift detected.')
                drift_detected_counter += 1
                drift_detected = True

            # save experiment in dictionary
            tmp_update_dict = {time_step: {'recalculated_db': recalculated_global_db,
                                           'drift_detected': drift_detected,
                                           'new_data_per_client': dict_new_data_per_client
                                           }
                               }

            dict_experiments_results['experiments'][repeat].update(tmp_update_dict)

    # output how often drift was detected
    print(f'Drift detected {drift_detected_counter} times out of {n_time_steps * n_repeats} iterations.')

    # add drift_detected_counter to dictionary
    dict_experiments_results['drift_detected_counter'] = drift_detected_counter

    # save experiment results as pickle file
    if save_results:
        now = datetime.datetime.now()
        now_str = now.strftime("%Y-%m-%d_%H-%M-%S")
        filename = f'./results/{run_mode}_{now_str}_results.pkl'
        with open(filename, 'wb') as f:
            pickle.dump(dict_experiments_results, f)
        print(f'Saved results to {filename}.')
    else:
        print('Not saving results.')
        filename = 'None'

    # log experiment results and parameters to mlflow
    if log_mlflow:
        import mlflow

        mlflow.set_tracking_uri('http://localhost:8080')
        mlflow.set_experiment(f'data_drift_experiments_{run_mode}')
        with mlflow.start_run():

            mlflow.log_param('run_mode', run_mode)
            mlflow.log_param('n_repeats', n_repeats)
            mlflow.log_param('n_time_steps', n_time_steps)
            mlflow.log_param('n_clients', n_clients)
            mlflow.log_param('ratio_drifted_clients', ratio_drifted_clients)
            mlflow.log_param('drifting_clients', drifting_clients)
            mlflow.log_param('acceptability_threshold', acceptability_threshold)
            mlflow.log_param('ratio_unused_client_generators', ratio_unused_client_generators)
            mlflow.log_param('n_unused_client_generators', n_unused_client_generators)
            mlflow.log_param('ratio_new_data_distribution', ratio_new_data_distribution)
            mlflow.log_param('dim_data', dim_data)
            mlflow.log_param('n_generators_per_client', n_generators_per_client)
            mlflow.log_param('initial_points_per_generator', initial_points_per_generator)
            mlflow.log_param('next_timestep_new_points_per_generator', next_timestep_new_points_per_generator)
            mlflow.log_param('mean_min_val', mean_min_val)
            mlflow.log_param('mean_max_val', mean_max_val)

            # mlflow.log_artifact(cov_mat_initial)
            mlflow.log_param('m', m)
            mlflow.log_param('tol_local', tol_local)
            mlflow.log_param('max_iter_local', max_iter_local)
            mlflow.log_param('max_iter_global', max_iter_global)
            mlflow.log_param('tol_global', tol_global)
            mlflow.log_param('ideal_cluster_num_deviation', ideal_cluster_num_deviation)
            mlflow.log_param('num_clusters_used', num_clusters)
            mlflow.log_param('ideal_cluster_num', ideal_cluster_num)
            mlflow.log_param('filename', filename)

            mlflow.log_metric('drift_detected_counter', drift_detected_counter)

            all_global_dbs = [
                dict_experiments_results.get('experiments').get(repeat).get('initial_cluster_results').get('global_db')
                for repeat in range(n_repeats)]

            mlflow.log_metric('initial_global_db_mean', np.mean(all_global_dbs))
            mlflow.log_metric('initial_global_db_median', np.median(all_global_dbs))
            mlflow.log_metric('initial_global_db_std', np.std(all_global_dbs))
            mlflow.log_metric('initial_global_db_min', np.min(all_global_dbs))
            mlflow.log_metric('initial_global_db_max', np.max(all_global_dbs))

            #mlflow.log_metric('initial_global_db', initial_global_db)
            # mlflow.log_artifact(dict_experiments_results)

            if run_mode in ['no_local_drift', 'local_but_no_global_drift']:
                false_positive_rate = drift_detected_counter / (n_repeats*n_time_steps)
                mlflow.log_metric('false_positive_rate', false_positive_rate)
            elif run_mode in ['global_drift', 'global_drift_unused_generators']:
                true_positive_rate = drift_detected_counter / (n_repeats*n_time_steps)
                mlflow.log_metric('true_positive_rate', true_positive_rate)

if __name__ == '__main__':

    ###### Set parameters for experiment ######
    logging.basicConfig(level=logging.INFO)

    save_results = True  # whether to save results dictionary as pickle file
    log_mlflow = True  # whether to log experiment results to mlflow

    n_repeats = 20  # number of times the experiment is repeated
    n_time_steps = 10  # number of time steps in each repeat

    # Create parameter grid for experiments
    parameter_grid = {
        'run_mode': ['global_drift_unused_generators'],#['global_drift', 'local_but_no_global_drift', 'global_drift_unused_generators'],
        'acceptability_threshold': [0.01, 0.025, 0.05],
        'dim_data': [2, 25, 100],
        'n_clients': [10],
        'ideal_cluster_num_deviation': [-0.5, 0.0, 0.5],
        'ratio_unused_client_generators': [0.1, 0.5, 0.9],
        'ratio_new_data_distribution': [0.1, 0.5, 0.9]
    }

    # iterate over parameter grid
    for run_mode in parameter_grid.get('run_mode'):
        for acceptability_threshold in parameter_grid.get('acceptability_threshold'):
            for dim_data in parameter_grid.get('dim_data'):
                for ideal_cluster_num_deviation in parameter_grid.get('ideal_cluster_num_deviation'):
                    for n_clients in parameter_grid.get('n_clients'):
                        # if the run_mode is 'global_drift_unused_generators', we only need to iterate over the ratio_unused_client_generators parameter
                        if run_mode == 'global_drift_unused_generators':
                            for ratio_unused_client_generators in parameter_grid.get('ratio_unused_client_generators'):
                                run_data_drift_experiments(run_mode=run_mode,
                                                           acceptability_threshold=acceptability_threshold,
                                                           dim_data=dim_data,
                                                           ideal_cluster_num_deviation=ideal_cluster_num_deviation,
                                                           ratio_unused_client_generators=ratio_unused_client_generators,
                                                           ratio_new_data_distribution=1.0, # this can also be used to make distribution "partially" disappear
                                                           n_repeats=n_repeats,
                                                           n_clients=n_clients,
                                                           n_time_steps=n_time_steps,
                                                           save_results=save_results,
                                                           log_mlflow=log_mlflow
                                                           )
                        # if the run_mode is 'global_drift', we need to iterate over the ratio_new_data_distribution parameters
                        elif run_mode == 'global_drift':
                            for ratio_new_data_distribution in parameter_grid.get('ratio_new_data_distribution'):
                                run_data_drift_experiments(run_mode=run_mode,
                                                           acceptability_threshold=acceptability_threshold,
                                                           dim_data=dim_data,
                                                           ideal_cluster_num_deviation=ideal_cluster_num_deviation,
                                                           ratio_unused_client_generators=0.0,
                                                           ratio_new_data_distribution=ratio_new_data_distribution,
                                                           n_repeats=n_repeats,
                                                           n_clients=n_clients,
                                                           n_time_steps=n_time_steps,
                                                           save_results=save_results,
                                                           log_mlflow=log_mlflow
                                                           )
                        # if the run_mode is 'local_but_no_global_drift' or 'no_local_drift', we do not need to iterate over any more parameters
                        else:
                            run_data_drift_experiments(run_mode=run_mode,
                                                       acceptability_threshold=acceptability_threshold,
                                                       dim_data=dim_data,
                                                       ideal_cluster_num_deviation=ideal_cluster_num_deviation,
                                                       n_repeats=20,
                                                       n_time_steps=10,
                                                       save_results=True,
                                                       log_mlflow=True
                                                       )